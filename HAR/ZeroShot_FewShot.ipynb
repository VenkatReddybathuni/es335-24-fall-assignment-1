{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to demonstrate Zero shot and Few shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from apikey import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq API and Models \n",
    "Groq_Token = api_key  # Do not share this key with anyone\n",
    "\n",
    "groq_models = {\"llama3-70b\": \"llama3-70b-8192\", \"mixtral\": \"mixtral-8x7b-32768\", \"gemma-7b\": \"gemma-7b-it\",\"llama3.1-70b\":\"llama-3.1-70b-versatile\",\"llama3-8b\":\"llama3-8b-8192\",\"llama3.1-8b\":\"llama-3.1-8b-instant\",\"gemma-9b\":\"gemma2-9b-it\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE : DO NOT SHARE THE API KEY WITH ANYONE. DO NOT COMMIT THE API KEY TO GITHUB.**\n",
    "\n",
    "Always do a sanity check before committing the code to github. If the key is found in the code, you will be penalized with a 0.5 marks deduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment label: Neutral\n",
      "\n",
      "Explanation: The sentence expresses mixed sentiments. The words \"amazing\" and \"happy\" convey a positive sentiment, indicating satisfaction with the product quality and customer service, respectively. However, the phrase \"delivery was delayed\" expresses a negative sentiment, indicating dissatisfaction with the delivery experience. Since both positive and negative sentiments are present, the overall sentiment is neutral.\n"
     ]
    }
   ],
   "source": [
    "# Statement \n",
    "sentence = \"The product quality is amazing but the delivery was delayed. However I am happy with the customer service.\"\n",
    "\n",
    "# System Prompts \n",
    "query = f\"\"\"\n",
    "* You are a sentiment analysis model. \n",
    "* Your task is to analyze the sentiment expressed in the given text and classify it as 'positive', 'negative', or 'neutral'. \n",
    "* Provide the sentiment label and, if necessary, a brief explanation of your reasoning.\n",
    "\n",
    "Sentence: {sentence}\n",
    "\"\"\" \n",
    "\n",
    "# To use Groq LLMs \n",
    "model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n",
    "answer = llm.invoke(query)\n",
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n",
      "\n",
      "Explanation: Although the sentence mentions a negative aspect (\"the delivery was delayed\"), the positive sentiments expressed in the sentence (\"The product quality is amazing\" and \"I am happy with the customer service\") outweigh the negative one, resulting in an overall positive sentiment.\n"
     ]
    }
   ],
   "source": [
    "# Statement \n",
    "sentence = \"The product quality is amazing but the delivery was delayed. However I am happy with the customer service.\"\n",
    "\n",
    "# System Prompts \n",
    "query = f\"\"\"\n",
    "* You are a sentiment analysis model. \n",
    "* Your task is to analyze the sentiment expressed in the given text and classify it as 'positive', 'negative', or 'neutral'. \n",
    "* Provide the sentiment label and, if necessary, a brief explanation of your reasoning.\n",
    "\n",
    "Here are few examples:\n",
    "1. Sentence: 'The customer service was excellent, and I received my order quickly.'\n",
    "Sentiment: Positive\n",
    "\n",
    "2. Sentence: 'The food was bland and the service was slow.'\n",
    "Sentiment: Negative\n",
    "\n",
    "3. Sentence: 'The product is okay, but it's not worth the price.'\n",
    "Sentiment: Neutral\n",
    "\n",
    "Sentence: {sentence}\n",
    "\"\"\" \n",
    "\n",
    "# To use Groq LLMs \n",
    "model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n",
    "answer = llm.invoke(query)\n",
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (126, 500, 3)\n",
      "Testing data shape:  (54, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "#\n",
    "#                                   ES335- Machine Learning- Assignment 1\n",
    "#\n",
    "# This file is used to create the dataset for the mini-project. The dataset is created by reading the data from\n",
    "# the Combined folder. The data is then split into training, testing, and validation sets. This split is supposed\n",
    "# to be used for all the modeling purposes.\n",
    "#\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# Library imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "time = 10\n",
    "offset = 100\n",
    "folders = [\"LAYING\",\"SITTING\",\"STANDING\",\"WALKING\",\"WALKING_DOWNSTAIRS\",\"WALKING_UPSTAIRS\"]\n",
    "classes = {\"WALKING\":1,\"WALKING_UPSTAIRS\":2,\"WALKING_DOWNSTAIRS\":3,\"SITTING\":4,\"STANDING\":5,\"LAYING\":6}\n",
    "\n",
    "combined_dir = os.path.join(\"Combined\")\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "                                                # Train Dataset\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "dataset_dir = os.path.join(combined_dir,\"Train\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir,folder))\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        df = pd.read_csv(os.path.join(dataset_dir,folder,file),sep=\",\",header=0)\n",
    "        df = df[offset:offset+time*50]\n",
    "        X_train.append(df.values)\n",
    "        y_train.append(classes[folder])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "                                                # Test Dataset\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "dataset_dir = os.path.join(combined_dir,\"Test\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir,folder))\n",
    "    for file in files:\n",
    "\n",
    "        df = pd.read_csv(os.path.join(dataset_dir,folder,file),sep=\",\",header=0)\n",
    "        df = df[offset:offset+time*50]\n",
    "        X_test.append(df.values)\n",
    "        y_test.append(classes[folder])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "                                                # Final Dataset\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# USE THE BELOW GIVEN DATA FOR TRAINING and TESTING purposes\n",
    "\n",
    "# concatenate the training and testing data\n",
    "X = np.concatenate((X_train,X_test))\n",
    "y = np.concatenate((y_train,y_test))\n",
    "\n",
    "# split the data into training and testing sets. Change the seed value to obtain different random splits.\n",
    "seed = 4\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=seed,stratify=y)\n",
    "\n",
    "print(\"Training data shape: \",X_train.shape)\n",
    "print(\"Testing data shape: \",X_test.shape)\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = X_train[0]\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='126'\n",
       "                  max='126',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  126\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='54'\n",
       "                  max='54',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  54\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tsfel\n",
    "from sklearn import preprocessing\n",
    "cfg = tsfel.get_features_by_domain() # retrieves all features\n",
    "X_train_featurised = tsfel.time_series_features_extractor(cfg, X_train, fs=50)\n",
    "X_test_featurised = tsfel.time_series_features_extractor(cfg, X_test, fs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 495)\n",
      "(126, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_train_reduced_tsefl = pca.fit_transform(X_train_featurised)\n",
    "print(X_train_featurised.shape)\n",
    "print(X_train_reduced_tsefl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Activity for example 1: 1\n",
      "Predicted Activity for example 2: 1\n",
      "Predicted Activity for example 3: 1\n",
      "Predicted Activity for example 4: 1\n",
      "Predicted Activity for example 5: 1\n",
      "Predicted Activity for example 6: 1\n",
      "Predicted Activity for example 7: 1\n",
      "Predicted Activity for example 8: 1\n",
      "Predicted Activity for example 9: 1\n",
      "Predicted Activity for example 10: 1\n",
      "Predicted Activity for example 11: 1\n",
      "Predicted Activity for example 12: 1\n",
      "Predicted Activity for example 13: 1\n",
      "Predicted Activity for example 14: 1\n",
      "Predicted Activity for example 15: 1\n",
      "Predicted Activity for example 16: 1\n",
      "Predicted Activity for example 17: 1\n",
      "Predicted Activity for example 18: 1\n",
      "Predicted Activity for example 19: 1\n",
      "Predicted Activity for example 20: 1\n",
      "Predicted Activity for example 21: 1\n",
      "Predicted Activity for example 22: 1\n",
      "Predicted Activity for example 23: 1\n",
      "Predicted Activity for example 24: 1\n",
      "Predicted Activity for example 25: 1\n",
      "Predicted Activity for example 26: 1\n",
      "Predicted Activity for example 27: 1\n",
      "Predicted Activity for example 28: 1\n",
      "Predicted Activity for example 29: 1\n",
      "Predicted Activity for example 30: 1\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# System Prompts \n",
    "query = \"\"\" \n",
    "You are tasked with classifying human activities based on featurized accelerometer data. The activities include:\n",
    "- WALKING\n",
    "- WALKING_UPSTAIRS\n",
    "- WALKING_DOWNSTAIRS\n",
    "- SITTING\n",
    "- STANDING\n",
    "- LAYING\n",
    "\n",
    "Now, given the following 495-feature vector representing an activity window, predict the most likely activity label:\n",
    "Feature Vector: {featues}\n",
    "\n",
    "\n",
    "do not give code.\n",
    "do not give extra information.\n",
    "return predicted activity only.\n",
    "\"\"\"\n",
    "\n",
    "# To use Groq LLMs \n",
    "model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n",
    "predicted_activities = []\n",
    "for i in range(30):  # Predict for 30 examples\n",
    "    example_vector = X_train_reduced_tsefl[i+3].tolist()  # Using different test examples\n",
    "    prompt = query.format(featues=example_vector)\n",
    "    predicted_activity = llm.invoke(prompt)\n",
    "    predicted_activities.append(classes[predicted_activity.content])\n",
    "    print(f\"Predicted Activity for example {i+1}: {classes[predicted_activity.content]}\")\n",
    "print(predicted_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1667\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array(predicted_activities)\n",
    "\n",
    "y_pred.shape\n",
    "accuracy = accuracy_score(y_test[3:33], y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Activity for example 1: 2\n",
      "Predicted Activity for example 2: 3\n",
      "Predicted Activity for example 3: 3\n",
      "Predicted Activity for example 4: 2\n",
      "Predicted Activity for example 5: 3\n",
      "Predicted Activity for example 6: 2\n",
      "Predicted Activity for example 7: 2\n",
      "Predicted Activity for example 8: 3\n",
      "Predicted Activity for example 9: 1\n",
      "Predicted Activity for example 10: 2\n",
      "Predicted Activity for example 11: 2\n",
      "Predicted Activity for example 12: 6\n",
      "Predicted Activity for example 13: 3\n",
      "Predicted Activity for example 14: 3\n",
      "Predicted Activity for example 15: 2\n",
      "Predicted Activity for example 16: 3\n",
      "Predicted Activity for example 17: 5\n",
      "Predicted Activity for example 18: 3\n",
      "Predicted Activity for example 19: 3\n",
      "Predicted Activity for example 20: 2\n",
      "Predicted Activity for example 21: 6\n",
      "Predicted Activity for example 22: 2\n",
      "Predicted Activity for example 23: 2\n",
      "Predicted Activity for example 24: 3\n",
      "Predicted Activity for example 25: 6\n",
      "Predicted Activity for example 26: 5\n",
      "Predicted Activity for example 27: 5\n",
      "Predicted Activity for example 28: 5\n",
      "Predicted Activity for example 29: 2\n",
      "Predicted Activity for example 30: 2\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\" \n",
    "You are tasked with classifying human activities based on featurized accelerometer data. The activities include:\n",
    "- WALKING\n",
    "- WALKING_UPSTAIRS\n",
    "- WALKING_DOWNSTAIRS\n",
    "- SITTING\n",
    "- STANDING\n",
    "- LAYING\n",
    "\n",
    "Here are a few labeled examples of the feature vectors and their corresponding activities:\n",
    "\n",
    "Example 1:\n",
    "Feature Vector: {example_1_vector}\n",
    "Activity: {example_1_label}\n",
    "\n",
    "Example 2:\n",
    "Feature Vector: {example_2_vector}\n",
    "Activity: {example_2_label}\n",
    "\n",
    "Example 3:\n",
    "Feature Vector: {example_3_vector}\n",
    "Activity: {example_3_label}\n",
    "\n",
    "Now, given the following 495-feature vector representing an activity window, predict the most likely activity label:\n",
    "Feature Vector: {feature_vector}\n",
    "\n",
    "\n",
    "do not give code.\n",
    "do not give extra information.\n",
    "return predicted activity only.\n",
    "\"\"\"\n",
    "\n",
    "# Few-shot learning function\n",
    "def few_shot_classification(feature_vector, labeled_examples):\n",
    "    # Prepare the prompt using a few labeled examples\n",
    "    prompt = few_shot_prompt.format(\n",
    "        example_1_vector=labeled_examples[0][\"feature_vector\"],\n",
    "        example_1_label=labeled_examples[0][\"label\"],\n",
    "        example_2_vector=labeled_examples[1][\"feature_vector\"],\n",
    "        example_2_label=labeled_examples[1][\"label\"],\n",
    "        example_3_vector=labeled_examples[2][\"feature_vector\"],\n",
    "        example_3_label=labeled_examples[2][\"label\"],\n",
    "        feature_vector=feature_vector\n",
    "    )\n",
    "    \n",
    "    # Invoke the Groq LLM\n",
    "    predicted_label = llm.invoke(prompt)\n",
    "    return predicted_label\n",
    "\n",
    "# Example labeled data for few-shot learning\n",
    "labeled_examples = [\n",
    "    {\"feature_vector\": X_train_reduced_tsefl[0].tolist(), \"label\": \"WALKING_UPSTAIRS\"},\n",
    "    {\"feature_vector\": X_train_reduced_tsefl[1].tolist(), \"label\": \"LAYING\"},\n",
    "    {\"feature_vector\": X_train_reduced_tsefl[2].tolist(), \"label\": \"STANDING\"},\n",
    "    {\"feature_vector\": X_train_reduced_tsefl[6].tolist(), \"label\": \"WALKING\"},\n",
    "    {\"feature_vector\": X_train_reduced_tsefl[16].tolist(), \"label\": \"WALKING_DOWNSTAIRS\"},\n",
    "    {\"feature_vector\": X_train_reduced_tsefl[9].tolist(), \"label\": \"SITTING\"},\n",
    "]\n",
    "predicted_activities = []\n",
    "for i in range(30):  # Predict for 30 examples\n",
    "    example_vector = X_train_reduced_tsefl[i + 3].tolist()  # Using different test examples\n",
    "    predicted_activity = few_shot_classification(example_vector, labeled_examples)\n",
    "    predicted_activities.append(classes[predicted_activity.content])\n",
    "    print(f\"Predicted Activity for example {i+1}: {classes[predicted_activity.content]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array(predicted_activities)\n",
    "\n",
    "y_pred.shape\n",
    "accuracy = accuracy_score(y_test[3:33], y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decision tree has more accuracy than fewshot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
